"use strict";(self.webpackChunkstaticdocs_starter=self.webpackChunkstaticdocs_starter||[]).push([[162],{3905:function(e,a,t){t.r(a),t.d(a,{MDXContext:function(){return s},MDXProvider:function(){return p},mdx:function(){return h},useMDXComponents:function(){return c},withMDXComponents:function(){return m}});var r=t(67294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(){return(o=Object.assign||function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var r in t)Object.prototype.hasOwnProperty.call(t,r)&&(e[r]=t[r])}return e}).apply(this,arguments)}function i(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function d(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?i(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,r,n=function(e,a){if(null==e)return{};var t,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var s=r.createContext({}),m=function(e){return function(a){var t=c(a.components);return r.createElement(e,o({},a,{components:t}))}},c=function(e){var a=r.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):d(d({},a),e)),t},p=function(e){var a=c(e.components);return r.createElement(s.Provider,{value:a},e.children)},u={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},_=r.forwardRef((function(e,a){var t=e.components,n=e.mdxType,o=e.originalType,i=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),m=c(t),p=n,_=m["".concat(i,".").concat(p)]||m[p]||u[p]||o;return t?r.createElement(_,d(d({ref:a},s),{},{components:t})):r.createElement(_,d({ref:a},s))}));function h(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var o=t.length,i=new Array(o);i[0]=_;var d={};for(var l in a)hasOwnProperty.call(a,l)&&(d[l]=a[l]);d.originalType=e,d.mdxType="string"==typeof e?e:n,i[1]=d;for(var s=2;s<o;s++)i[s]=t[s];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}_.displayName="MDXCreateElement"},60333:function(e,a,t){t.r(a),t.d(a,{contentTitle:function(){return l},default:function(){return p},frontMatter:function(){return d},metadata:function(){return s},toc:function(){return m}});var r=t(83117),n=t(80102),o=(t(67294),t(3905)),i=["components"],d={sidebar_position:1,id:"examples",title:"Examples"},l="Examples",s={unversionedId:"howto/examples",id:"howto/examples",isDocsHomePage:!1,title:"Examples",description:"Introduction",source:"@site/docs/howto/examples.md",sourceDirName:"howto",slug:"/howto/examples",permalink:"/Aria_data_tools/docs/howto/examples",editUrl:"https://github.com/facebookresearch/aria_data_tools/docs/howto/examples.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,id:"examples",title:"Examples"},sidebar:"tutorialSidebar",previous:{title:"Sensors and Measurements",permalink:"/Aria_data_tools/docs/sensors-measurements"},next:{title:"Accessing Sensor Data",permalink:"/Aria_data_tools/docs/howto/dataprovider"}},m=[{value:"Introduction",id:"introduction",children:[]},{value:"Retrieve and Read Data Using the Project Aria Data Provider",id:"retrieve-and-read-data-using-the-project-aria-data-provider",children:[{value:"Python",id:"python",children:[]},{value:"C++",id:"c",children:[]}]},{value:"Visualizing Sequences and Pre-Computed Camera Trajectory",id:"visualizing-sequences-and-pre-computed-camera-trajectory",children:[{value:"Python",id:"python-1",children:[]},{value:"C++",id:"c-1",children:[]}]}],c={toc:m};function p(e){var a=e.components,d=(0,n.Z)(e,i);return(0,o.mdx)("wrapper",(0,r.Z)({},c,d,{components:a,mdxType:"MDXLayout"}),(0,o.mdx)("h1",{id:"examples"},"Examples"),(0,o.mdx)("h2",{id:"introduction"},"Introduction"),(0,o.mdx)("p",null,"The Aria Research Kit: Aria Data Tools provides Python3 code and a C++ library to work with ",(0,o.mdx)("a",{parentName:"p",href:"/Aria_data_tools/docs/aria-vrs"},"VRS files"),"."),(0,o.mdx)("p",null,"The following examples provide specific scenarios for how to use the Project Aria Data Provider and how to visualize sequences and pre-computed camera trajectory."),(0,o.mdx)("h2",{id:"retrieve-and-read-data-using-the-project-aria-data-provider"},"Retrieve and Read Data Using the Project Aria Data Provider"),(0,o.mdx)("h3",{id:"python"},"Python"),(0,o.mdx)("p",null,"In this example, records are extracted and read for the Left SLAM Camera from ",(0,o.mdx)("inlineCode",{parentName:"p"},"recording.vrs"),".\nFor more information about the Project Aria Data Provider go to ",(0,o.mdx)("a",{parentName:"p",href:"/Aria_data_tools/docs/howto/dataprovider"},"Accessing Sensor Data")),(0,o.mdx)("h4",{id:"1-open-and-select-the-file-to-read"},"1. Open and select the file to read"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"$ python\n>>> import pyark.datatools as datatools\n>>> vrs_data_provider = datatools.dataprovider.AriaVrsDataProvider()\n>>> vrs_data_provider.openFile(\u2018recording.vrs\u2019)\n")),(0,o.mdx)("h4",{id:"2-select-which-sensor-information-to-extract"},"2. Select which sensor information to extract"),(0,o.mdx)("p",null,"Either through a high-level API:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"vrs_data_provider.setSlamLeftCameraPlayer()\n")),(0,o.mdx)("p",null,"or with a StreamID:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},">>> slam_camera_recordable_type_id = 1201\n>>> slam_left_camera_instance_id = 1\n>>> slam_left_camera_stream_id = datatools.dataprovider.StreamId(slam_camera_recordable_type_id, slam_left_camera_instance_id)\n>>> vrs_data_provider.setStreamPlayer(slam_left_camera_stream_id)\n")),(0,o.mdx)("h4",{id:"3--set-whether-to-print-data-layouts-optional"},"3.  Set whether to print data layouts (optional)"),(0,o.mdx)("p",null,"By default, data layouts are not printed while reading records. Set the verbosity to True to print data layouts and False to not print data layouts:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"vrs_data_provider.setVerbose(True)\n")),(0,o.mdx)("h4",{id:"4-read-the-data-stream"},"4. Read the data stream"),(0,o.mdx)("p",null,"All records in timestamp order, example command and output."),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},">>> vrs_data_provider.readAllRecords()\n4822.486 Camera Data (SLAM) #1 [1201-1]: jpg, 44338 bytes. # JPEG compressed image data size before decompression\n...\n4832.286 Camera Data (SLAM) #1 [1201-1]: jpg, 64148 bytes.\n4832.386 Camera Data (SLAM) #1 [1201-1]: jpg, 64174 bytes.\n")),(0,o.mdx)("p",null,"Read a single data record by timestamp:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"vrs_data_provider.readDataRecordByTime(slam_left_camera_stream_id, someTimestamp)\n")),(0,o.mdx)("p",null,"You can also use a higher level API that reads a data record in a specific stream and proceeds next timestamp in the player internally."),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"vrs_data_provider.tryFetchNextData(slam_left_camera_stream_id)\n")),(0,o.mdx)("h4",{id:"5-access-the-data-stream"},"5. Access the data stream"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},">>> slam_left_camera_player = vrs_data_provider.getSlamLeftCameraPlayer()\n>>> slam_left_camera_data_record = slam_left_camera_player.getDataRecord()\n>>> slam_left_camera_data_record.captureTimestampNs\n 4832385508212\n\n>>> slam_left_camera_data = slam_left_camera_player.getData()\n>>> pixel_frame = slam_left_camera_data.pixelFrame\n>>> buffer = pixel_frame.getBuffer()\n>>> len(buffer)\n307200 # JPEG image data decompressed internally in AriaImageSensorPlayer\n# equal to SLAM camera image width (640) * image height(480)\n\n")),(0,o.mdx)("h4",{id:"6-read-the-first-configuration-record-of-a-stream"},"6. Read the first configuration record of a stream:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"vrs_data_provider.readFirstConfigurationRecord(slam_left_camera_stream_id)\n")),(0,o.mdx)("h4",{id:"7-load-the-device-model"},"7. Load the device model"),(0,o.mdx)("p",null,"There are calibration strings for each image and motion stream. Reading the configuration record for any one of them will load the device model."),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},">>> slam_left_camera_stream_id = slam_left_camera_player.getStreamId()\n>>> slam_left_camera_stream_id\n<pyark.datatools.dataprovider.StreamId object at 0x7f955808c270>\n>>> vrs_data_provider.readFirstConfigurationRecord(slam_left_camera_stream_id)\nTrue\n>>> vrs_data_provider.loadDeviceModel()\nTrue\n>>> device_model = vrs_data_provider.getDeviceModel()\n>>> device_model\n<pyark.datatools.sensors.DeviceModel object at 0x7f955808c2b0>\n")),(0,o.mdx)("h3",{id:"c"},"C++"),(0,o.mdx)("p",null,"Use the following commands to verbosely read all or some data streams in a Project Aria VRS file."),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"$ cd build/data_provider\n$ ./read_all <vrs_path> # Read records of all streams verbosely\n$ ./read_selected <vrs_path> # Read records of selected streams verbosely\n")),(0,o.mdx)("h2",{id:"visualizing-sequences-and-pre-computed-camera-trajectory"},"Visualizing Sequences and Pre-Computed Camera Trajectory"),(0,o.mdx)("p",null,"Use the following commands to run the Visualization Tool:"),(0,o.mdx)("h3",{id:"python-1"},"Python"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"$ cd src/visualization\n$ python3.9 main.py ${vrs_path} ${optional_pose_path} ${optional_eyetracking_path} ${optional_speechtotext_path}\n")),(0,o.mdx)("h3",{id:"c-1"},"C++"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"$ cd build/visualization\n$ ./aria_viewer ${vrs_path} ${optional_pose_path} ${optional_eyetracking_path} ${optional_speechtotext_path}\n\n")),(0,o.mdx)("p",null,"Pose, eye tracking and speech2text paths are optional. If they are not provided, the visualization tool will try to infer the full paths of ",(0,o.mdx)("inlineCode",{parentName:"p"},"trajectory.csv"),", ",(0,o.mdx)("inlineCode",{parentName:"p"},"et_in_rgb_stream.csv")," and ",(0,o.mdx)("inlineCode",{parentName:"p"},"speech_aria_domain.csv")," from the VRS path by using the Aria Pilot Dataset structure:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"\u251c\u2500\u2500 location\n\u2502 \u2514\u2500\u2500 trajectory.csv\n\u251c\u2500\u2500 eyetracking\n\u2502 \u2514\u2500\u2500 et_in_rgb_stream.csv\n\u251c\u2500\u2500 speech2text\n\u2502 \u2514\u2500\u2500 speech_aria_domain.csv\n\u251c\u2500\u2500 ${vrs_path}\n\u2514\u2500\u2500 ...\n")),(0,o.mdx)("p",null,"You will see something like the image below in the viewer after you press the ",(0,o.mdx)("strong",{parentName:"p"},"Play")," button.\n",(0,o.mdx)("img",{alt:"img image of visualization tool",src:t(18768).Z})),(0,o.mdx)("p",null,"For more information, about this tool, go to ",(0,o.mdx)("a",{parentName:"p",href:"/Aria_data_tools/docs/howto/visualizing"},"Visualize Sequences and Pre-Computed Camera Trajectory")))}p.isMDXComponent=!0},18768:function(e,a,t){a.Z=t.p+"assets/images/aria_viewer-f6f0cd7e503ac9f794b4a9f1d4d2142b.png"}}]);